---
title: 'Modeling and Prediction of Credit Scores'
authors: "Sharad Sambhwani, Hamdan Siddiqui,Peter Hu"
date: "December 14, 2016"
output:
  html_document:
    theme: readable
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# STAT 420 Final Project Group Members

- Sharad Sambhwani (sharads2@illinois.edu)
- Hamdan Siddiqui (abulhs2@illinois.edu)
- Peter Hu (zeliang3@illinois.edu)


# Introduction

## Description of dataset

The dataset *credit.csv* consists of a number of numeric and categorical variables relevant to a person's credit score.

 The numeric variables are:
 
- Income: in thousands of dollars
- Rating: the Credit score rating
- Balance: Average credit card debt for a number of individuals
- Education: Years of education
- Limit: Credit Card Limit
- Cards: Number of credit cards owned


The categorical variables are:


- Gender {Male, Female}
- Married {Yes, No}
- Ethnicity {African American, Asian, Caucasian}
- Student {Yes, No}

## Background information of dataset

This dataset is available as part of the book "An Introduction to Statistical Learning, with applications in R"  (Springer, 2013) and can be downloaded from http://www-bcf.usc.edu/~gareth/ISL/Credit.csv

## Motivation

A credit score in the United States is a number representing the creditworthiness of a person, the likelihood that person will pay his or her debts. Lenders, such as banks and credit card companies, use credit scores to evaluate the potential risk posed by lending money to consumers. Widespread use of credit scores has made credit more widely available and less expensive for many consumers


The FICO score was first introduced in 1989 by FICO, then called Fair, Isaac, and Company. The FICO model is used by the vast majority of banks and credit grantors, and is based on consumer credit files of the three national credit bureaus: Experian, Equifax, and TransUnion.


Banks and Lenders are always interested in models that can accurately predict the credit score rating. It is natural to assume a linear relationship between the predictor variable balance and the response variable rating. However, other variables such as limit (credit card limit), income, marital status and education may have some interaction with balance.


In this study, we would like to explore the following:

- Multiple regression models between the response variable $rating$ and the other variables in the dataset
- Which variables are more important than others and can lead to a good prediction of a person's credit card score
- Which categorical variables interact with the numeric variables?
- Which model choice is acceptable? Provide enough justification
- Use the chosen model to explain relationships and predict observations.


```{r, message=FALSE, warning= FALSE, echo=FALSE}
library(broom)
library(MASS)
library(leaps)
library(faraway)
library(dplyr)
library(lmtest)
```


```{r}
credit = read.csv("Credit.csv")
str(credit)
head(credit)

```


```{r, echo=FALSE}
# Generic plotting functions
plot_fitted_residuals = function(fit, pointcol, linecol) {
  plot(fitted(fit), resid(fit), col = pointcol, xlab = "Fitted", ylab = "Residuals")
  abline(h = 0, col = linecol, lwd = 2)
}

plot_normal_qq_residuals = function(fit, pointcol, linecol){
  qqnorm(resid(fit), main = "Normal Q-Q Plot, fit", col = pointcol)
  qqline(resid(fit), col = linecol, lwd = 2)
}

```

```{r, echo=FALSE}
rmse  = function(actual, predicted) {
  sqrt(mean((actual - predicted) ^ 2))
}
```


```{r, echo=FALSE}
loocv_rmse = function(model) {
  sqrt(mean((resid(model) / (1 - hatvalues(model))) ^ 2))
}
```


# Methods

## Modification of Dataset

The original dataset contains numeric variables $Limit$ and $Balance$. In determining the credit score or rating of an individual, [How FICO scores looks at credit card limits](http://www.myfico.com/crediteducation/articles/fico_scores_credit_limit.aspx),  the credit limit by itself is not considered directly. Instead, the FICO score considers your credit limit when determining your "credit utilization rate." Utilization means the amount of your available credit that you are using at the time your score is calculated. It is calculated by dividing an account's outstanding balance by its credit limit. Credit utilization rate has proven to be extremely predictive of future repayment risk. So it is often an important factor in a person's score. Generally speaking, the higher your utilization rate is, the greater is the risk that you will default on a credit account within the next two years.

As a result, the dataset has been modified by eliminating the variables $Limit$ and $Balance$ and instead replacing it by a new variable $CreditUtilization$ which is the ratio of $Balance$ to $Limit$

```{r, echo = FALSE}
credit["CreditUtil"] = credit$Balance/credit$Limit # Create a new variable = Balance/Limit
credit_mod = within(credit, rm(Limit, Balance)) # Get rid of Limit and Balance variables

credit_mod_numeric = select_if(credit_mod, is.numeric) # Only contains numeric variables

```

```{r}
head(credit_mod)

```



## Basic Plots

In the following, we perform elementary statistical analysis on the modified dataset in the form of

- histograms of the numeric variables
- simple linear regression
    - between the response variable $Rating$ and the numeric predictor variables
    - Fitted vs. residual plots for both numeric and categorical predictor variables
- boxplots of the categorical variables

### Histograms

Below, we plot the histograms of the numeric variables of the modified dataset.

Here are some observations:

- The distributions of Rating and Income are right-skewed.
- There is an odd spike around 0 in the distribution of Credit Utilization. 
- The distribution of Age is close to being symmetric.
- The distribution of Education is left skewed which is heartening to see.

```{r, echo = FALSE}
par(mfrow = c(2, 3), oma = c(0, 0, 5, 0))

hist(credit_mod$Rating,
  main = "Histogram of Rating",
  xlab = "Rating",
  col = "dodgerblue",
  border = "darkorange")
abline(v=mean(credit_mod$Rating), col = "black", lwd = 2)


hist(credit_mod$Income,
  main = "Histogram of Income",
  xlab = "Income",
  col = "dodgerblue",
  border = "darkorange")
abline(v=mean(credit_mod$Income), col = "black", lwd = 2)

hist(credit_mod$CreditUtil,
  main = "Histogram of Credit Utilization",
  xlab = "Income",
  col = "dodgerblue",
  border = "darkorange")
abline(v=mean(credit_mod$CreditUtil), col = "black", lwd = 2)

hist(credit_mod$Age,
  main = "Histogram of Age",
  xlab = "Age",
  col = "dodgerblue",
  border = "darkorange")
abline(v=mean(credit_mod$Age), col = "black", lwd = 2)

hist(credit_mod$Education,
  main = "Histogram of Education",
  xlab = "Education",
  col = "dodgerblue",
  border = "darkorange")
abline(v=mean(credit_mod$Education), col = "black", lwd = 2)

mtext(expression('Histogram of Numeric Variables '), outer = TRUE, cex = 1.3)

```


### Simple Linear Regression


```{r, echo = FALSE}
slr_rating_income = lm(Rating~Income, data = credit_mod)
slr_rating_credit_util = lm(Rating~CreditUtil, data = credit_mod)
slr_rating_education = lm(Rating~Education, data = credit_mod)
slr_rating_age = lm(Rating~Age, data = credit_mod)
slr_rating_cards = lm(Rating~Cards, data = credit_mod)
slr_rating_student = lm(Rating~Student, data = credit_mod)
slr_rating_gender = lm(Rating~Gender, data = credit_mod)
slr_rating_ethnicity = lm(Rating~Ethnicity, data = credit_mod)
slr_rating_married = lm(Rating~Married, data = credit_mod)

```

Below we plot the fitted regression lines between the response variable $Rating$ and each of the numeric predictor variables:

- For the SLR due to Income and Credit Utilization predictor variables, the fitted regression lines have a non-trivial positive slope
- For the SLR due to Age and Cards predictor variables, the fitted regression line has a slightly positive slope
- For the SLR due to Education, the fitted regression line has a slightly negative slope.
- In all cases, the actual values are more or less symmetric around the fitted values. 



```{r, echo = FALSE}
par(mfrow = c(2, 3), oma = c(0, 0, 5, 0))

plot(Rating~Income, data = credit_mod,
     main = "Rating vs. Income",
     pch = 20,
     cex = 2,
     cex.lab = 1.3,
     cex.main = 1.3,
     col = "red")
abline(slr_rating_income, lwd = 2, col = "black")

plot(Rating~CreditUtil, data = credit_mod,
     main = "Rating vs. Credit Utilization",
     pch = 20,
     cex = 2,
     cex.lab = 1.3,
     cex.main = 1.3,
     col = "red")
abline(slr_rating_credit_util, lwd = 2, col = "black")



plot(Rating~Education, data = credit_mod,
     main = "Rating vs. Education",
     pch = 20,
     cex = 2,
     cex.lab = 1.3,
     cex.main = 1.3,
     col = "red")
abline(slr_rating_education, lwd = 2, col = "black")

plot(Rating~Age, data = credit_mod,
     main = "Rating vs. Age",
     pch = 20,
     cex = 2,
     cex.lab = 1.3,
     cex.main = 1.3,
     col = "red")
abline(slr_rating_age, lwd = 2, col = "black")

plot(Rating~Cards, data = credit_mod,
     main = "Rating vs. Cards",
     pch = 20,
     cex = 2,
     cex.lab = 1.3,
     cex.main = 1.3,
     col = "red")
abline(slr_rating_cards, lwd = 2, col = "black")



mtext(expression('Rating ~ Numeric Variables '), outer = TRUE, cex = 1.3)

```


As part of the simple linear regression analysis, We also examine the fitted vs. residual plots below for the numeric predictor variables:

- The linearity assumption appears to hold true for all the cases since the residuals are equally scattered around 0.
- The constant variance assumption appears to hold true when Income, Education and Cards are the predictor variables while it appears to be borderline for the Age predictor variable
- The constant variance assumption is clearly violated for the Credit Utilization predictor variable. The spread clearly varies as a function of fitted values.



```{r, echo = FALSE}
par(mfrow = c(2, 3), oma = c(0, 0, 5, 0))


plot(fitted(slr_rating_income), resid(slr_rating_income), col = "dodgerblue",
     pch = 20, cex = 1.5, xlab = "Fitted", ylab = "Residuals", main = "Rating vs. Income")
abline(h = 0, lty = 2, col = "darkorange", lwd = 2)


plot(fitted(slr_rating_credit_util), resid(slr_rating_credit_util), col = "dodgerblue",
     pch = 20, cex = 1.5, xlab = "Fitted", ylab = "Residuals", main = "Rating vs. Credit Utilization")
abline(h = 0, lty = 2, col = "darkorange", lwd = 2)

plot(fitted(slr_rating_education), resid(slr_rating_education), col = "dodgerblue",
     pch = 20, cex = 1.5, xlab = "Fitted", ylab = "Residuals", main = "Rating vs. Educaction")
abline(h = 0, lty = 2, col = "darkorange", lwd = 2)

plot(fitted(slr_rating_age), resid(slr_rating_age), col = "dodgerblue",
     pch = 20, cex = 1.5, xlab = "Fitted", ylab = "Residuals", main = "Rating vs. Age")
abline(h = 0, lty = 2, col = "darkorange", lwd = 2)

plot(fitted(slr_rating_cards), resid(slr_rating_cards), col = "dodgerblue",
     pch = 20, cex = 1.5, xlab = "Fitted", ylab = "Residuals", main = "Rating vs. Cards")
abline(h = 0, lty = 2, col = "darkorange", lwd = 2)



mtext(expression('Fitted ~ Residuals (Numeric Variables) '), outer = TRUE, cex = 1.3)

```

We also examine the fitted vs. residual plots below for the categorical predictor variables. In all cases, the linearity and constant variance assumptions appear to hold true.



```{r, echo = FALSE}
par(mfrow = c(2, 2), oma = c(0, 0, 5, 0))



plot(fitted(slr_rating_student), resid(slr_rating_student), col = "dodgerblue",
     pch = 20, cex = 1.5, xlab = "Fitted", ylab = "Residuals", main = "Rating vs. Student")
abline(h = 0, lty = 2, col = "darkorange", lwd = 2)

plot(fitted(slr_rating_education), resid(slr_rating_ethnicity), col = "dodgerblue",
     pch = 20, cex = 1.5, xlab = "Fitted", ylab = "Residuals", main = "Rating vs. Ethnicity")
abline(h = 0, lty = 2, col = "darkorange", lwd = 2)

plot(fitted(slr_rating_married), resid(slr_rating_married), col = "dodgerblue",
     pch = 20, cex = 1.5, xlab = "Fitted", ylab = "Residuals", main = "Rating vs. Married")
abline(h = 0, lty = 2, col = "darkorange", lwd = 2)

plot(fitted(slr_rating_gender), resid(slr_rating_gender), col = "dodgerblue",
     pch = 20, cex = 1.5, xlab = "Fitted", ylab = "Residuals", main = "Rating vs. Gender")
abline(h = 0, lty = 2, col = "darkorange", lwd = 2)


mtext(expression('Fitted ~ Residuals (Categorical Variables) '), outer = TRUE, cex = 1.3)

```

### Boxplots

The boxplots are provided below for the $Rating$ response variable as a function of each of the 4 categorical variables ($Ethnicity$, $Student$, $Gender$ and $Married$). A key observation is that for each of these variables, the statistics do not seem to be sensitive to the category values.

```{r, echo = FALSE}


par( mfrow = c(1, 4), oma = c(0, 0, 5, 0))

plot(Rating ~ Ethnicity, data = credit, col = 2:4, main = "Rating vs. Ethnicity")
plot(Rating ~ Student, data = credit, col = 2:3, main = "Rating vs. Student")
plot(Rating ~ Gender, data = credit, col = 2:3, main = "Rating vs. Gender")
plot(Rating ~ Married, data = credit, col = 2:3, main = "Rating vs. Married")

mtext(expression('Boxplots of Rating vs. Categorical Variables '), outer = TRUE, cex = 1.3)


```


```{r, echo = FALSE}
rating_ethn_aov = aov(Rating ~ Ethnicity, data = credit)
rating_student_aov = aov(Rating ~ Student, data = credit)
rating_married_aov = aov(Rating ~ Married, data = credit)
rating_gender_aov = aov(Rating ~ Gender, data = credit)

```



#### One-way ANOVA

In the following, we use ANOVA tests to test for equality of means between different groups of each of the categorical variables

The p-values for the ANOVA tests using each of the categorical variables as a predictor are:

- Ethnicity: `r glance(rating_ethn_aov)$p.value`
- Student: `r glance(rating_student_aov)$p.value`
- Married: `r glance(rating_married_aov)$p.value`
- Gender: `r glance(rating_gender_aov)$p.value`

Since the p-value of the above tests for each of the categorical variables is incredibly high, using any reasonable significance level we would accept the null hypothesis i.e. for each of the categorical variables the means are the same for each group.


## Linear Regression

### Check of correlations between predictors

Recall that correlation measures strength and direction of the linear relationship between two variables.

We first perform a quick check of correlation between the predictors. The plot below plots all possible scatterplots between pairs of variables in the modified dataset.

```{r}
pairs(credit_mod, col = "red")

```

We can also do this numerically with the cor() function, which when applied to a dataset, returns all pairwise correlations. Notice this is a symmetric matrix.  As observed above and below, the correlation between any pair of the numeric predictor variables (Income, CreditUtil, Age, Education, Cards) is quite low.

```{r, echo=FALSE}

round(cor(credit_mod_numeric), 2)


```



```{r, echo= FALSE}

mlr_rating_all = lm(Rating ~ ., data = credit_mod)
mlr_rating_inc_credit_util = lm(Rating ~ Income + CreditUtil, data = credit_mod)
mlr_rating_inc_credit_util_stud = lm(Rating ~ Income + CreditUtil + Student, data = credit_mod)
mlr_rating_ethnicity = lm(Rating~Ethnicity, data = credit)



```

### Check for multicollinearity

The variance inflation factor quantifies the effect of collinearity on the variance of our regression estimates.The vif function from the faraway package calculates the VIFs for each of the predictors of a model.

```{r}
vif(mlr_rating_all)

```

We conclude that there is no multicollinearity issue as all the predictors have a VIF much less than 5.


### Significance of Regression Tests for Linear Regression

The significance of regression test between 2 models is based on the F-statistic. The p-values when we test for significance using the ANOVA test between a pair of models are as follows:

- SLR based on Income vs. MLR based on Income and Credit Utilization: **`r format(tidy(anova(slr_rating_income, mlr_rating_inc_credit_util))[2,]$p.value, scientific = TRUE) `**
- SLR based on Credit Utilization vs. MLR based on Income and Credit Utilization: **`r format(tidy(anova(slr_rating_credit_util, mlr_rating_inc_credit_util))[2,]$p.value, scientific = TRUE)`**
- MLR based on Income and Credit Utilization vs. MLR based on Income, Credit Utilization and Student: **`r format(tidy(anova(mlr_rating_inc_credit_util, mlr_rating_inc_credit_util_stud))[2,]$p.value,scientific = TRUE)`**
- MLR based on Income, Credit Utilization and Student vs. MLR based on all predictors: **`r round(tidy(anova(mlr_rating_inc_credit_util_stud, mlr_rating_all))[2,]$p.value, 3)`**

Based on the above, for a significance level less than or equal to 2%, the multi regression model based on Income, Credit Utilization and Student is significant amongst all the models including the additive model based on all predictor variables.





### Goodness of fit based on the coefficient of determination ($R^2$)

The coefficient of determination is interpreted as the proportion of observed variation in the response  that can be explained by the linear regression model. It is a statistical measure of how close the data are to the fitted regression.

Below is the R-squared metric for different regression models

- SLR based on Income: **`r round(summary(slr_rating_income)$r.squared, 4) `**
- SLR based on Credit Utilization: **`r round(summary(slr_rating_credit_util)$r.squared, 4) `**
- MLR based on Income and Credit Utilization: **`r round(summary(mlr_rating_inc_credit_util)$r.squared, 4) `**
- MLR based on Income, Credit Utilization and Student: **`r round(summary(mlr_rating_inc_credit_util_stud)$r.squared, 4) `**
- MLR based on all predictors: **`r round(summary(mlr_rating_all)$r.squared, 4)`**

As seen above, based on R-squared metric alone, a good model choice would be the one based on Income, Credit Utilization and Student, since it uses only 4 coefficients and it is very close in value to the additive model based on all predictors.

So, we see that based on both a) Significant tests and b) R-squared criteria, the additive model based on Income, Credit Utilization and Student is a good model choice.


### Reduction in Std. Deviation of Error

The standard deviation of error of the observed samples $Rating$ corresponds to a very simple model $Y_i = \beta + \epsilon_i$ resulting in the Rating estimator to be the sample mean $\hat{y} = \bar{y}$. In other words, in the absence of applying a regression model, we assume the null hypothesis i.e. none of the predictors are useful.

On the other hand, if we apply a regression model, the estimate of $Rating$ has a much lower standard deviation of error compared to the simple model that assumes no regression (null hypothesis).


- $s_y = `r sd(credit_mod$Rating)`$ gives us an estimate of the variability of Rating. Specifically how the observed Rating data varies about its mean. (We could think of this as an estimate for how the observations vary in the model $Y_i = \beta_0 +\epsilon_i$.) This estimate does not use the predictors in any way.
- The standard error $s_e$ gives us an estimate of the variability of the residuals of the model, specifically how the observed $Rating$ data varies about the fitted regression. This estimate does take into account the predictors.


In the following, we evaluate the standard error $s_e$ for the following regression models:


- SLR based on Income: **`r round(summary(slr_rating_income)$sigma,3)`**
- SLR based on Credit Utilization: **`r round(summary(slr_rating_credit_util)$sigma,3) `**
- MLR based on Income, Credit Utilization and Student: **`r round(summary(mlr_rating_inc_credit_util_stud)$sigma,3)`**
- MLR based on all predictors: **`r round(summary(mlr_rating_all)$sigma,3) `**

As seen above, while the variability of the estimate of the response variable $Rating$ around the fiited regression reduces for the simple regression models based on Income as well as Credit Utilization, there is a drastic reduction in variability due to MLR based on Income, Credit Utilization and Student. Furthermore, this variability is very close to that observed for the MLR based on all predictors which happens to be the lowest amongst all.



### Analysis of Model Coefficients:

#### Multiple Linear Regression model based on Income, Credit Utilization and Student

##### Estimated coefficients $\hat{\beta_i}$



- $\hat{\beta_0} = `r round(coef(mlr_rating_inc_credit_util_stud)[1],3)`$ is the estimated Rating for a an income of $0, a credit utilization rate of 0, and an individual who is not a student 
- $\hat{\beta_1} = `r round(coef(mlr_rating_inc_credit_util_stud)[2],3)`$ is the estimated change in mean Rating for an increase of $1 in income for a certain credit utilization and student type
- $\hat{\beta_2} = `r round(coef(mlr_rating_inc_credit_util_stud)[3],3)`$ is the estimated change in mean Rating for an increase of 1 in credit utilization for a certain income and student type
- $\hat{\beta_3} = `r round(coef(mlr_rating_inc_credit_util_stud)[4],3)`$ is the estimated change in mean Rating for a student with a certain income and credit utilization.


##### Confidence interval estimates of coefficients $\hat{\beta_i}$

The 90% confidence interval estimate of the linear regression coefficients $\hat{\beta_i}$ is given as

```{r, echo=FALSE}

confint(mlr_rating_inc_credit_util_stud, level = 0.90)

```


# Results

## Exploring Explanation Models

In the following, we explore reasonably "small" models that are good at explaining the relationship between the response and the predictors.

### Adding Interactions

Using the additive model based on Income, Credit Utilization and Student as a starting point, we investigate further the effect of two-way and three-way interactions between these variables.

```{r, echo=FALSE}
mlr_rating_inc_credit_util_stud_inter = lm(Rating ~ Income*CreditUtil + CreditUtil*Student + Income*Student + Income*CreditUtil*Student, data = credit_mod)



```


#### Significance of Regression

We now test for significance between the two hypotheses:

- $H_0$: Rating is a linear estimate of Income, Credit Utilization and Student predictor variables
- $H_1$: Rating is a linear estimate of Income, Credit Utilization, Student and the interaction terms between these predictor variables

Since the p-value(**`r format(tidy(anova(mlr_rating_inc_credit_util_stud, mlr_rating_inc_credit_util_stud_inter))[2,]$p.value, scientific =TRUE)`**) is incredibly low, we reject the null hypothesis and accept the alternate hypothesis.

#### Improvement in coefficient of determination ($R^2$)

We also investigate the improvement in the coefficient of determination due to interactions.
Compared to the value **`r round(summary(mlr_rating_inc_credit_util_stud)$r.squared, 4) `** obtained for the additive model based on Income, Credit Utilization and Student, if we include the interaction terms, the $R^2$ value improves to **`r round(summary(mlr_rating_inc_credit_util_stud_inter)$r.squared, 4)`**.

Given the high value $R^2$ and the low number of coefficients (length(coef(mlr_rating_inc_credit_util_stud_inter))) in this model, we conclude that this model serves as a good explanation model.


#### Checking Model Assumptions

##### Fitted vs. Residuals plot

In the following, we check the linearity, normality and equal variance assumptions of the model based on Income, Credit Utilization, Student and the interaction terms. It is critical to check for these assumptions to ensure that the inference made on the Significance of Regression tests are valid.

In a fitted vs. residual plot, we typically look for the following:

- At any fitted value, the mean of the residuals should be roughly 0. If this is the case, the linearity assumption is valid. For this reason, we generally add a horizontal line at  y=0 to emphasize this point.
- At every fitted value, the spread of the residuals should be roughly the same. If this is the case, the constant variance assumption is valid.




```{r, echo=FALSE}
plot_fitted_residuals(mlr_rating_inc_credit_util_stud_inter,"dodgerblue", "darkorange")
plot_normal_qq_residuals(mlr_rating_inc_credit_util_stud_inter,"dodgerblue", "darkorange")

```


Based on the above plots and applying the Breusch-Pagan and Shapiro-Wilk tests we conclude the following:

- The linearity assumption is not violated due to the following:
    - From the fitted versus residual plot, the residuals seem roughly centered at 0
    - The mean value of the residuals across all fitted values is `r round((mean(resid(mlr_rating_inc_credit_util_stud_inter))),3)`
- The constant variance assumption is not violated due to the following:
    - From the fitted vs. residual plot, for all the fitted values, the spread of the residuals appears to be constant across all fitted values
        - Note that the p-value (`r format(bptest(mlr_rating_inc_credit_util_stud_inter)$p.value, scientific = TRUE)`) resulting from the Breusch-Pagan test is very low. This is probably due to a few outliers in the observations.
- The normal distribution assumption for the errors is not violated due to the following:
    - The Q-Q plot clearly show that the points of the plot closely follow a straight line, thereby suggesting that the data does come from a normal distribution.
    - In the Shapiro-Wilk test, the null hypothesis assumes the data follows a normal distribution. Since the p-value(`r round(shapiro.test(residuals(mlr_rating_inc_credit_util_stud_inter))$p.value, 2)`) is high, we fail to reject the null hypothesis and conclude that the data follows a normal distribution.



#### Improving Breusch-Pagan test results

One possible reason, why the Breusch-Pagan test revealed a very low p-value and hence violation of the constant variance assumption could be due to a few outliers in the observations.



```{r, echo=FALSE}
mlr_rating_inter_student_yes = lm(Rating ~ Income*CreditUtil + CreditUtil + Income + Income*CreditUtil, data = credit_mod, subset = (Student == "Yes"))

mlr_rating_inter_student_no = lm(Rating ~ Income*CreditUtil + CreditUtil + Income + Income*CreditUtil, data = credit_mod, subset = (Student == "No"))

```

We first analyze a subset of the observations corresponding to individuals who are students. For this group, if we run the the Breusch-Pagan test, the p-value is sufficiently high (**`r round(bptest(mlr_rating_inter_student_yes)$p.value,2)`**) leading to the decision that the errors have constant variance about the true model for the group of students.

However, in this dataset, the number of individuals who are students are in the minority (`r nrow(subset(credit_mod, Student == "Yes"))` out of `r nrow(credit_mod)`). For the group who are not students, if we run the Breusch-Pagan test on them, the p-value still remains very low (**`r format(bptest(mlr_rating_inter_student_no)$p.value, scientific = TRUE)` **) 

Another group to consider is the set of people whose credit utilization rate is very low or very high.


```{r, echo=FALSE}
mlr_rating_inter_tails = lm(Rating ~ Income*CreditUtil + CreditUtil*Student + Income*Student + Income*CreditUtil*Student, data = credit_mod, subset = ((CreditUtil < 0.015) |  (CreditUtil > 0.18)))
```

When we run the the Breusch-Pagan test on such a group, the p-value is sufficiently high (**`r round(bptest(mlr_rating_inter_tails)$p.value,2)`**) leading to the decision that the errors have constant variance about the true model for the group of students.The size of this group is `r nrow(subset(credit_mod, (CreditUtil < 0.015) |  (CreditUtil> 0.18)))` which is roughly 30% of the total number of observations.


#### Outlier Analysis

Observations that have a large effect on regression are referred to as influential observations. These correspond to points of high leverage and large residual.

```{r, echo=FALSE}

cd_rating_inter = cooks.distance(mlr_rating_inc_credit_util_stud_inter)
large_rating = cd_rating_inter > 4 / length(cd_rating_inter)


```

Based on the Cook's distance measure, the following observations are considered to be influential

```{r}
credit_mod[large_rating, ]

```


We then refit the interaction multiple regression model without any points identified as influential. 

```{r}

mlr_rating_inc_credit_util_stud_inter_fix = lm(Rating ~ Income*CreditUtil + CreditUtil*Student + Income*Student + Income*CreditUtil*Student, data = credit_mod, subset = cd_rating_inter > 4 / length(cd_rating_inter) )

```


As seen below, there is a non-trival change in coefficients between the fitted model based on all points and the fitted model based on all points minus the influential points.

```{r}
coef(mlr_rating_inc_credit_util_stud_inter)
coef(mlr_rating_inc_credit_util_stud_inter_fix)

```


Finally, we create a data frame that stores the observations that were "removed" because they were influential and Use the two models we have fit to make predictions with these observations. 

```{r}
credit_mod.sub = subset(credit_mod, cd_rating_inter > 4/ length(cd_rating_inter))
rating_pred_inter = predict(mlr_rating_inc_credit_util_stud_inter, credit_mod.sub)
rating_pred_inter_fix = predict(mlr_rating_inc_credit_util_stud_inter_fix, credit_mod.sub)

rating_pred_inter
rating_pred_inter_fix

```

The predicted rating due to the model that is based on non-influential points is consistently higher  than the predicted rating due to the model that was based on all points for all the influential observations.

#### Performing transformations

As a means of stabilizing the variance of errors assumed in a model, variance stabilizing transformations could also be considered. 

First, we need to test if we need do the transformation for the response variable.

```{r,echo=FALSE}
full = lm(Rating ~ . , data = credit_mod)
boxcox(full, lambda = seq(0.25, 1.5, by = 0.1), plotit = TRUE)
```

Using the Box-Cox method, we see that $\lambda = 0.8$ is both in the confidence interval, and is extremely close to the maximum, which suggests a transformation of the form $\frac{y^\lambda - 1}{\lambda} = \frac{y^{0.8} - 1}{0.8}$.
As for the predictor transformation, from the graphs above, we think it is unnecessary. 

```{r, echo=FALSE}
mlr_rating_inc_credit_util_stud_transformation = lm((Rating ^ 0.8) ~ Income*CreditUtil + CreditUtil*Student + Income*Student + Income*CreditUtil*Student, data = credit_mod)

```


We now test the transformation model. The plots below are very similar to the ones corresponding to the non-transformation model suggesting that the linearity and normality assumptions are still met while not much improvement in the constant variance assumption. The $R^2$ value of this model is **`r round(summary(mlr_rating_inc_credit_util_stud_transformation)$r.squared, 4)`**, a little bit smaller than the previous non-transformation model **`r round(summary(mlr_rating_inc_credit_util_stud_inter)$r.squared, 4)`**. And the p-value of Breusch-Pagan test is (**`r format(bptest(mlr_rating_inc_credit_util_stud_transformation)$p.value, scientific = TRUE)` **), also worse than the non-transformation model. So for the explanation model, we think the non-transformation model based on interactions of Income, Credit Utilization and Student predictor variables may still be the best. 

```{r,echo=FALSE}

# plot the residual and the qq-graph
plot_fitted_residuals(mlr_rating_inc_credit_util_stud_transformation,"dodgerblue", "darkorange")
plot_normal_qq_residuals(mlr_rating_inc_credit_util_stud_transformation,"dodgerblue", "darkorange")
```




## Exploring Predictive Models

Here, we explore models that have small errors and hence are good for making predictions. To find models for prediction, we use the selection criterion that implicitly penalizes larger models, such as the leave-one-out cross-validated RMSE (LOOCV RMSE). So long as the model does not over-fit, we do not actually care how large the model becomes. Explaining the relationship between the variables is not our goal here and we don't need to worry about model assumptions.


For each of the following models we perform a step search to obtain the best model based on the AIC and BIC criteria:

1. An additive model that includes all the predictor variables. 
2. A model based on all possible 2-way interactions of the predictor variables.
3. A model based on all possible 3-way interactions of the predictor variables
4. A model based on 3rd order polynomial terms of the Income and CreditUtil variables
5. A model based on all possible 2-way interactions and 3rd order polynomial terms of the Income and CreditUtil variables

In the case of model 2, two variants of step search were performed:

- In the first variant, no scope argument was provided.
- In the second variant, a single formula was provided in the scope argument that represented the 9-way interaction between all the predictor variables:
    - Income, Cards, Age, Education, Gender, Student, Married, Ethnicity and CreditUtil



```{r, echo=FALSE}
rating_add = lm(Rating ~ ., data = credit_mod)
rating_add_back_aic = step(rating_add, direction = "backward", trace = 0)

n = length(resid(rating_add))
rating_add_back_bic = step(rating_add, direction = "backward", k = log(n), trace = 0)


```



```{r, echo=FALSE}

rating_inter = lm(Rating ~ .^2, data = credit_mod)
rating_inter2_1_both_aic = step(rating_inter, direction = "both", trace = 0)

n = length(resid(rating_inter))
rating_inter2_1_both_bic = step(rating_inter, direction = "both", k = log(n), trace = 0)

```

```{r, echo=FALSE}

rating_inter2_2_both_aic = step(rating_inter,Rating ~ Income*Cards*Age*Education*Gender*Student*Married*Ethnicity*CreditUtil, direction = "both", trace = 0)


```



```{r, echo=FALSE}

rating_inter3 = lm(Rating ~ .^3 , data = credit_mod)
rating_inter3_both_aic = step(rating_inter3, direction = "both", trace = 0)

```




```{r, echo=FALSE}
rating_poly3 =  lm(Rating ~  . - Income - CreditUtil + poly(Income, 3) + poly(CreditUtil,3) , data = credit_mod)
rating_poly3_back_aic = step(rating_poly3, direction = "backward", trace = 0)
n = length(resid(rating_poly3))
rating_poly3_back_bic = step(rating_poly3, direction = "backward", k = log(n), trace = 0)
```



```{r, echo=FALSE}
rating_inter_poly3 =  lm(Rating ~  (. ) ^ 2  - Income - CreditUtil + poly(Income, 3) + poly(CreditUtil, 3) , data = credit_mod)
rating_inter_poly3_back_aic = step(rating_inter_poly3, direction = "backward", trace = 0)
n = length(resid(rating_inter_poly3))
rating_inter_poly3_back_bic = step(rating_inter_poly3, direction = "backward", k = log(n), trace = 0)
```






```{r, echo=FALSE}
LOOCV = c(
  loocv_rmse(rating_add_back_aic),
  loocv_rmse(rating_inter2_1_both_bic),
  loocv_rmse(rating_inter2_2_both_aic),
  loocv_rmse(rating_inter3_both_aic),
  loocv_rmse(rating_poly3_back_aic),
  loocv_rmse(rating_inter_poly3_back_aic)
  
)

coef_length = c(
  length(coef(rating_add_back_aic)),
  length(coef(rating_inter2_1_both_bic)),
  length(coef(rating_inter2_2_both_aic)),
  length(coef(rating_inter3_both_aic)),
  length(coef(rating_poly3_back_aic)),
  length(coef(rating_inter_poly3_back_aic))
)

```





Table 1 provides a summary of the LOOCV RMSE achieved and the number of regression coefficients for each of the models considered. As seen in the Table, the step-wise search based on the 2-way interaction and third order polynomial terms of the Income and Credit Utilization predictor variables achieves the lowest LOOCV RMSE.

**Table 1: Comparing LOOCV RMSE for different fitted models**

| Model                                             | LOOCV RMSE      | Number of Coefficients  |
|---------------------------------------------------|-----------------|-------------------------|
| Additive (BIC)                                    |`r LOOCV[1]`     |`r coef_length[1]`       |
| Two-way interaction (BIC)                         |`r LOOCV[2]`     |`r coef_length[2]`       |
| Two-way interaction w/ scope (AIC)                |`r LOOCV[3]`     |`r coef_length[3]`       |
| Three-way interaction (AIC)                       |`r LOOCV[4]`     |`r coef_length[4]`       |
| Third order polynomial (AIC)                      |`r LOOCV[5]`     |`r coef_length[5]`       |
| Two-way interaction & Third order polynomial(AIC) |**`r LOOCV[6]`** |**`r coef_length[6]`**       |





# Discussion

In this study, we investigated the problem of modeling and prediction of a person's credit score as a function of multiple variables such as a person's income, credit card limit, outstanding balance, etc. An elementary statistical analysis was performed on each of these variables to determine the distribution (histograms, boxplots) of these variables and metrics (estimation coefficients, mean and variance of residuals ) related to simple linear regression. Several techniques studied throughout STAT420 as appropriate were applied here in this study.

The dataset was slightly modified to introduce a new predictor variable related to credit utilization rate as a function of balance and credit limit as this variable is directly used in determining a person's credit score. A key finding upfront was that credit score can be well explained by a linear combination of three important predictor variables (Income, Credit Utilization rate and whether the person is a student or not). In fact, it was shown that such a model is a significant when compared to a model based on all predictors for levels of significance less than or equal to 2%. An analysis of $\hat{\beta_i}$ coefficients and confidence interval estimates was also obtained for this model. 

Next, explanation models were explored from the point of view of maximizing $R^2$ and using as few coefficients as possible. Towards this objective, a model based on 2-way and 3-way interaction between the Income, Credit Utlization and Student predictor variables was obtained leading to $R^2$ = **`r round(summary(mlr_rating_inc_credit_util_stud_inter)$r.squared, 4)`** and utilization of **`r length(coef(mlr_rating_inc_credit_util_stud_inter))`** coefficients. 

Further checks were performed on the explanation model assumptions for linearity, constant variance and normality. An exercise was also performed to identify subsets of the data that led to a favorable decision on the constant variance assumption via the Breusch-Pagan test. Also, an outlier analysis was performed on this model to assess the sensitivity of regression coefficients to influential observations and the prediction of $Rating$ estimate had we used a model that was trained on data without these influential observations. 

Finally we also explored a few candidate models to determine the best predictive model using LOOCV RMSE as a metric. Methods based on step-wise search using the AIC and BIC criteria were used for this purpose. Amongst the candidates considered here, a model derived based on 2-way interactions and third order terms using **`r coef_length[6]`** coefficients was shown to achieve the lowest LOOCV RMSE (**`r round(LOOCV[6], 3)`** )


